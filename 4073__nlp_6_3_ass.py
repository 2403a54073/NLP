# -*- coding: utf-8 -*-
"""4073__nlp_6.3 ass

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CYWyektb0D26gOOP4x98cQg3B5NeIqsC
"""

import pandas as pd

df = pd.read_csv('/content/arxiv_data.csv')

print("First 5 rows of the DataFrame:")
print(df.head())

print("\nColumn names of the DataFrame:")
print(df.columns)

text_data = df['summaries']

print("First 5 entries of the text data:")
print(text_data.head())

print("\nNumber of missing values in text data:")
print(text_data.isnull().sum())

import sys
!{sys.executable} -m pip install nltk
print("nltk installed.")

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

# Initialize the WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# Create a list of English stopwords
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # a. Convert the text to lowercase
    text = text.lower()
    # b. Remove non-alphabetic characters
    text = re.sub(r'[^a-z]', ' ', text)
    # c. Tokenize the text into individual words
    words = text.split()
    # d. Remove stopwords from the tokenized words
    words = [word for word in words if word not in stop_words]
    # e. Lemmatize the remaining words
    words = [lemmatizer.lemmatize(word) for word in words]
    # f. Join the processed words back into a single string
    return ' '.join(words)

# Apply this preprocessing function to the text_data Series
cleaned_text = text_data.apply(preprocess_text)

# Print the first 5 entries of the cleaned_text
print("First 5 entries of the cleaned text:")
print(cleaned_text.head())

# Step 1: Import CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Assume cleaned_text is a list of preprocessed text documents
# Example: cleaned_text = ["this is a sample document", "this document is another example"]

# Step 2: Initialize CountVectorizer with parameters
vectorizer = CountVectorizer(max_df=0.9, min_df=5)

# Step 3: Fit and transform the cleaned_text data
bow_matrix = vectorizer.fit_transform(cleaned_text)

# Step 4: Print shape and first 10 feature names
print("Shape of BoW matrix:", bow_matrix.shape)
print("First 10 feature names:", vectorizer.get_feature_names_out()[:10])

from sklearn.feature_extraction.text import CountVectorizer

# Initialize CountVectorizer with specified parameters
# max_df=0.9 ignores terms that appear in more than 90% of the documents
# min_df=5 ignores terms that appear in less than 5 documents
vectorizer = CountVectorizer(max_df=0.9, min_df=5)

# Fit and transform the cleaned_text data to create the Bag of Words matrix
bow_matrix = vectorizer.fit_transform(cleaned_text)

# Print the shape of the resulting BoW matrix
print("Shape of Bag of Words matrix:", bow_matrix.shape)

# Get the feature names (words)
feature_names = vectorizer.get_feature_names_out()

# Print the first 10 feature names
print("First 10 feature names:", feature_names[:10])

from sklearn.decomposition import LatentDirichletAllocation

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx + 1}:")
        # Get indices of words with highest probabilities for this topic
        top_words_indices = topic.argsort()[:-no_top_words - 1:-1]
        # Get the actual words using the indices
        top_words = [feature_names[i] for i in top_words_indices]
        print(f"  {', '.join(top_words)}")

# Initialize LDA model
lda_model = LatentDirichletAllocation(n_components=5, random_state=42) # You can adjust n_components

# Fit LDA model to the bow_matrix (from arxiv_data.csv as per prior steps)
lda_model.fit(bow_matrix)

# Display the top 10 words for each topic
display_topics(lda_model, feature_names, 10)

import numpy as np

# Transform the bow_matrix using the fitted lda_model to get document-topic distributions
document_topic_distributions = lda_model.transform(bow_matrix)

# Print the shape of the document-topic distributions
print("Shape of document-topic distributions:", document_topic_distributions.shape)

import numpy as np

# Identify the dominant topic for each document
# np.argmax finds the index of the maximum probability along axis=1 (for each row/document)
dominant_topics = np.argmax(document_topic_distributions, axis=1)

# Add the 'dominant_topic' column to the original DataFrame df
df['dominant_topic'] = dominant_topics

# Display the first few rows of the updated DataFrame with the new column
print("First 5 rows of the DataFrame with 'dominant_topic' column:")
print(df.head())

import pandas as pd

# Load dataset
df = pd.read_csv("/content/news.csv") # Corrected file path

# Select text column
texts = df['News'].astype(str) # Corrected column name to 'News'

print(texts.head())

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    # Lowercase
    text = text.lower()

    # Remove special characters & numbers
    text = re.sub(r'[^a-z\s]', '', text)

    # Tokenization
    tokens = text.split()

    # Stopword removal & lemmatization
    tokens = [lemmatizer.lemmatize(word)
              for word in tokens if word not in stop_words]

    # Rejoin
    return ' '.join(tokens)

df['clean_text'] = texts.apply(preprocess)

print(df['clean_text'].head())

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
bow = vectorizer.fit_transform(df['clean_text'])

bow_df = pd.DataFrame(
    bow.toarray(),
    columns=vectorizer.get_feature_names_out()
)

print(bow_df.head())

from sklearn.decomposition import LatentDirichletAllocation

lda = LatentDirichletAllocation(
    n_components=2,      # number of topics
    random_state=42
)

lda.fit(bow)

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"\nTopic {topic_idx + 1}:")
        print(" ".join([
            feature_names[i]
            for i in topic.argsort()[:-no_top_words - 1:-1]
        ]))

display_topics(lda, vectorizer.get_feature_names_out(), 5)

topic_values = lda.transform(bow)
df['topic'] = topic_values.argmax(axis=1)

print(df[['News', 'topic']])

"""NMF

"""

import pandas as pd
import re
import nltk

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import pandas as pd

df = pd.read_csv('/content/arxiv_data.csv', engine='python', on_bad_lines='warn')
texts = df['summaries'].astype(str)

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]
    return " ".join(tokens)

df['clean_text'] = texts.apply(preprocess)

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(max_features=1000)
bow_matrix = vectorizer.fit_transform(df['clean_text'])

bow_df = pd.DataFrame(
    bow_matrix.toarray(),
    columns=vectorizer.get_feature_names_out()
)

bow_df.head()

from sklearn.decomposition import NMF

nmf_model = NMF(
    n_components=5,
    random_state=42
)

W = nmf_model.fit_transform(bow_matrix)
H = nmf_model.components_

words = vectorizer.get_feature_names_out()

for topic_idx, topic in enumerate(H):
    print(f"\nTopic {topic_idx + 1}:")
    print([words[i] for i in topic.argsort()[-10:]])

df['Topic'] = W.argmax(axis=1)
df[['News', 'Topic']].head()

import pandas as pd

df = pd.read_csv('/content/news.csv')
texts = df['News'].astype(str)

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]
    return " ".join(tokens)

df['clean_text'] = texts.apply(preprocess)

from sklearn.feature_extraction.text import CountVectorizer

# Ensure 'clean_text' is available in the current DataFrame
# The 'texts' variable and 'preprocess' function should be available from previous successful executions.
df['clean_text'] = texts.apply(preprocess)

vectorizer = CountVectorizer(max_features=1000)
bow_matrix = vectorizer.fit_transform(df['clean_text'])

bow_df = pd.DataFrame(
    bow_matrix.toarray(),
    columns=vectorizer.get_feature_names_out()
)

bow_df.head()

# Apply NMF
from sklearn.decomposition import NMF
import numpy as np

nmf_model = NMF(n_components=5, random_state=42)

W = nmf_model.fit_transform(bow_matrix)   # Document–Topic matrix
H = nmf_model.components_                # Topic–Word matrix

# ---- OUTPUTS ----

# 1. Model
print("NMF Model:")
print(nmf_model)

# 2. Shape of W
print("\nShape of W (Documents × Topics):")
print(W.shape)

# 3. Shape of H
print("\nShape of H (Topics × Words):")
print(H.shape)

# 4. Sample values of W
print("\nSample W values (first 3 documents):")
print(W[:3])

# 5. Sample values of H
print("\nSample H values (first topic, first 10 words):")
print(H[0][:10])

# 6. Dominant topic for each document
dominant_topics = np.argmax(W, axis=1)
print("\nDominant topics (first 10 documents):")
print(dominant_topics[:10])

feature_names = vectorizer.get_feature_names_out()

for topic_idx, topic in enumerate(H):
    print(f"\nTopic {topic_idx + 1}:")
    print([feature_names[i] for i in topic.argsort()[-10:]])

# The error indicates a mismatch in the number of rows between 'df' and 'W'.
# 'W' was generated from the 'news.csv' data (4 rows), but 'df' currently holds 'arxiv_data.csv' data (51774 rows).
# To align them, we'll re-load the 'news.csv' into 'df' to ensure consistency for this operation.
import pandas as pd

df = pd.read_csv('/content/news.csv')
# Assuming 'News' column is present in news.csv and was used to create 'W'

df['Topic'] = W.argmax(axis=1)
df[['News', 'Topic']].head()