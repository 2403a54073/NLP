{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNt8ZXqF5yJuU3d375P4F82",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403a54073/NLP/blob/main/4073_assignment_5_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.Using Nltk\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ozFunIgpr5W4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8102b2f",
        "outputId": "dfddc5e7-5c7f-4a3a-eb16-f62a452c8fd3"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "ac665b12",
        "outputId": "a11b1bbc-855a-49c3-89be-5e9bbe5c3b23"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "display(df.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \n",
              "0           ['cs.CV', 'cs.LG']  \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
              "2           ['cs.CV', 'cs.AI']  \n",
              "3                    ['cs.CV']  \n",
              "4           ['cs.CV', 'cs.LG']  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-190d276d-d043-4e93-87d6-af7f337b3af7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-190d276d-d043-4e93-87d6-af7f337b3af7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-190d276d-d043-4e93-87d6-af7f337b3af7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-190d276d-d043-4e93-87d6-af7f337b3af7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging\",\n          \"Background-Foreground Segmentation for Interior Sensing in Automotive Industry\",\n          \"Enforcing Mutual Consistency of Hard Regions for Semi-supervised Medical Image Segmentation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.\",\n          \"To ensure safety in automated driving, the correct perception of the\\nsituation inside the car is as important as its environment. Thus, seat\\noccupancy detection and classification of detected instances play an important\\nrole in interior sensing. By the knowledge of the seat occupancy status, it is\\npossible to, e.g., automate the airbag deployment control. Furthermore, the\\npresence of a driver, which is necessary for partially automated driving cars\\nat the automation levels two to four can be verified. In this work, we compare\\ndifferent statistical methods from the field of image segmentation to approach\\nthe problem of background-foreground segmentation in camera based interior\\nsensing. In the recent years, several methods based on different techniques\\nhave been developed and applied to images or videos from different\\napplications. The peculiarity of the given scenarios of interior sensing is,\\nthat the foreground instances and the background both contain static as well as\\ndynamic elements. In data considered in this work, even the camera position is\\nnot completely fixed. We review and benchmark three different methods ranging,\\ni.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural\\nnetwork, namely a Mask R-CNN. In particular, the limitations of the classical\\nmethods, GMM and Morphological Snakes, for interior sensing are shown.\\nFurthermore, it turns, that it is possible to overcome these limitations by\\ndeep learning, e.g.\\\\ using a Mask R-CNN. Although only a small amount of ground\\ntruth data was available for training, we enabled the Mask R-CNN to produce\\nhigh quality background-foreground masks via transfer learning. Moreover, we\\ndemonstrate that certain augmentation as well as pre- and post-processing\\nmethods further enhance the performance of the investigated methods.\",\n          \"In this paper, we proposed a novel mutual consistency network (MC-Net+) to\\neffectively exploit the unlabeled hard regions for semi-supervised medical\\nimage segmentation. The MC-Net+ model is motivated by the observation that deep\\nmodels trained with limited annotations are prone to output highly uncertain\\nand easily mis-classified predictions in the ambiguous regions (e.g. adhesive\\nedges or thin branches) for the image segmentation task. Leveraging these\\nregion-level challenging samples can make the semi-supervised segmentation\\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\\nof two new designs. First, the model contains one shared encoder and multiple\\nsightly different decoders (i.e. using different up-sampling strategies). The\\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, a new\\nmutual consistency constraint is enforced between one decoder's probability\\noutput and other decoders' soft pseudo labels. In this way, we minimize the\\nmodel's uncertainty during training and force the model to generate invariant\\nand low-entropy results in such challenging areas of unlabeled data, in order\\nto learn a generalized feature representation. We compared the segmentation\\nresults of the MC-Net+ with five state-of-the-art semi-supervised approaches on\\nthree public medical datasets. Extension experiments with two common\\nsemi-supervised settings demonstrate the superior performance of our model over\\nother existing methods, which sets a new state of the art for semi-supervised\\nmedical image segmentation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"['cs.CV', 'cs.AI', 'cs.LG']\",\n          \"['cs.CV']\",\n          \"['cs.CV', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "186d2fef",
        "outputId": "bf51c148-8809-4db9-fcf0-3b4a01589956"
      },
      "source": [
        "def preprocess_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|https\\S+|www\\S+', '', text)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove social media mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove hashtags\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove emojis (comprehensive pattern)\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\" # Start of character group\n",
        "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE\n",
        "    )\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    # Remove special characters (keep alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['processed_summaries'] = df['summaries'].apply(preprocess_text)\n",
        "display(df[['summaries', 'processed_summaries']].head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                                 processed_summaries  \n",
              "0  stereo matching is one of the widely used tech...  \n",
              "1  the recent advancements in artificial intellig...  \n",
              "2  in this paper we proposed a novel mutual consi...  \n",
              "3  consistency training has proven to be an advan...  \n",
              "4  to ensure safety in automated driving the corr...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0975ae1-ef6f-4b44-9b84-8a33e2d59ff9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summaries</th>\n",
              "      <th>processed_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>stereo matching is one of the widely used tech...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>the recent advancements in artificial intellig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>in this paper we proposed a novel mutual consi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>consistency training has proven to be an advan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>to ensure safety in automated driving the corr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0975ae1-ef6f-4b44-9b84-8a33e2d59ff9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c0975ae1-ef6f-4b44-9b84-8a33e2d59ff9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c0975ae1-ef6f-4b44-9b84-8a33e2d59ff9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df[['summaries', 'processed_summaries']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.\",\n          \"To ensure safety in automated driving, the correct perception of the\\nsituation inside the car is as important as its environment. Thus, seat\\noccupancy detection and classification of detected instances play an important\\nrole in interior sensing. By the knowledge of the seat occupancy status, it is\\npossible to, e.g., automate the airbag deployment control. Furthermore, the\\npresence of a driver, which is necessary for partially automated driving cars\\nat the automation levels two to four can be verified. In this work, we compare\\ndifferent statistical methods from the field of image segmentation to approach\\nthe problem of background-foreground segmentation in camera based interior\\nsensing. In the recent years, several methods based on different techniques\\nhave been developed and applied to images or videos from different\\napplications. The peculiarity of the given scenarios of interior sensing is,\\nthat the foreground instances and the background both contain static as well as\\ndynamic elements. In data considered in this work, even the camera position is\\nnot completely fixed. We review and benchmark three different methods ranging,\\ni.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural\\nnetwork, namely a Mask R-CNN. In particular, the limitations of the classical\\nmethods, GMM and Morphological Snakes, for interior sensing are shown.\\nFurthermore, it turns, that it is possible to overcome these limitations by\\ndeep learning, e.g.\\\\ using a Mask R-CNN. Although only a small amount of ground\\ntruth data was available for training, we enabled the Mask R-CNN to produce\\nhigh quality background-foreground masks via transfer learning. Moreover, we\\ndemonstrate that certain augmentation as well as pre- and post-processing\\nmethods further enhance the performance of the investigated methods.\",\n          \"In this paper, we proposed a novel mutual consistency network (MC-Net+) to\\neffectively exploit the unlabeled hard regions for semi-supervised medical\\nimage segmentation. The MC-Net+ model is motivated by the observation that deep\\nmodels trained with limited annotations are prone to output highly uncertain\\nand easily mis-classified predictions in the ambiguous regions (e.g. adhesive\\nedges or thin branches) for the image segmentation task. Leveraging these\\nregion-level challenging samples can make the semi-supervised segmentation\\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\\nof two new designs. First, the model contains one shared encoder and multiple\\nsightly different decoders (i.e. using different up-sampling strategies). The\\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, a new\\nmutual consistency constraint is enforced between one decoder's probability\\noutput and other decoders' soft pseudo labels. In this way, we minimize the\\nmodel's uncertainty during training and force the model to generate invariant\\nand low-entropy results in such challenging areas of unlabeled data, in order\\nto learn a generalized feature representation. We compared the segmentation\\nresults of the MC-Net+ with five state-of-the-art semi-supervised approaches on\\nthree public medical datasets. Extension experiments with two common\\nsemi-supervised settings demonstrate the superior performance of our model over\\nother existing methods, which sets a new state of the art for semi-supervised\\nmedical image segmentation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"the recent advancements in artificial intelligence ai combined with the extensive amount of data generated by todays clinical systems has led to the development of imaging ai solutions across the whole value chain of medical imaging including image reconstruction medical image segmentation imagebased diagnosis and treatment planning notwithstanding the successes and future potential of ai in medical imaging many stakeholders are concerned of the potential risks and ethical implications of imaging ai solutions which are perceived as complex opaque and difficult to comprehend utilise and trust in critical clinical applications despite these concerns and risks there are currently no concrete guidelines and best practices for guiding future ai developments in medical imaging towards increased trust safety and adoption to bridge this gap this paper introduces a careful selection of guiding principles drawn from the accumulated experiences consensus and best practices from five large european projects on ai in health imaging these guiding principles are named futureai and its building blocks consist of i fairness ii universality iii traceability iv usability v robustness and vi explainability in a stepbystep approach these guidelines are further translated into a framework of concrete recommendations for specifying developing evaluating and deploying technically clinically and ethically trustworthy ai solutions into clinical practice\",\n          \"to ensure safety in automated driving the correct perception of the situation inside the car is as important as its environment thus seat occupancy detection and classification of detected instances play an important role in interior sensing by the knowledge of the seat occupancy status it is possible to eg automate the airbag deployment control furthermore the presence of a driver which is necessary for partially automated driving cars at the automation levels two to four can be verified in this work we compare different statistical methods from the field of image segmentation to approach the problem of backgroundforeground segmentation in camera based interior sensing in the recent years several methods based on different techniques have been developed and applied to images or videos from different applications the peculiarity of the given scenarios of interior sensing is that the foreground instances and the background both contain static as well as dynamic elements in data considered in this work even the camera position is not completely fixed we review and benchmark three different methods ranging ie gaussian mixture models gmm morphological snakes and a deep neural network namely a mask rcnn in particular the limitations of the classical methods gmm and morphological snakes for interior sensing are shown furthermore it turns that it is possible to overcome these limitations by deep learning eg using a mask rcnn although only a small amount of ground truth data was available for training we enabled the mask rcnn to produce high quality backgroundforeground masks via transfer learning moreover we demonstrate that certain augmentation as well as pre and postprocessing methods further enhance the performance of the investigated methods\",\n          \"in this paper we proposed a novel mutual consistency network mcnet to effectively exploit the unlabeled hard regions for semisupervised medical image segmentation the mcnet model is motivated by the observation that deep models trained with limited annotations are prone to output highly uncertain and easily misclassified predictions in the ambiguous regions eg adhesive edges or thin branches for the image segmentation task leveraging these regionlevel challenging samples can make the semisupervised segmentation model training more effective therefore our proposed mcnet model consists of two new designs first the model contains one shared encoder and multiple sightly different decoders ie using different upsampling strategies the statistical discrepancy of multiple decoders outputs is computed to denote the models uncertainty which indicates the unlabeled hard regions second a new mutual consistency constraint is enforced between one decoders probability output and other decoders soft pseudo labels in this way we minimize the models uncertainty during training and force the model to generate invariant and lowentropy results in such challenging areas of unlabeled data in order to learn a generalized feature representation we compared the segmentation results of the mcnet with five stateoftheart semisupervised approaches on three public medical datasets extension experiments with two common semisupervised settings demonstrate the superior performance of our model over other existing methods which sets a new state of the art for semisupervised medical image segmentation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2da065bf",
        "outputId": "21869495-1466-4edd-da3e-c3eb5bb5e6a6"
      },
      "source": [
        "df['tokenized_summaries'] = df['processed_summaries'].apply(nltk.word_tokenize)\n",
        "display(df[['processed_summaries', 'tokenized_summaries']].head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                 processed_summaries  \\\n",
              "0  stereo matching is one of the widely used tech...   \n",
              "1  the recent advancements in artificial intellig...   \n",
              "2  in this paper we proposed a novel mutual consi...   \n",
              "3  consistency training has proven to be an advan...   \n",
              "4  to ensure safety in automated driving the corr...   \n",
              "\n",
              "                                 tokenized_summaries  \n",
              "0  [stereo, matching, is, one, of, the, widely, u...  \n",
              "1  [the, recent, advancements, in, artificial, in...  \n",
              "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
              "3  [consistency, training, has, proven, to, be, a...  \n",
              "4  [to, ensure, safety, in, automated, driving, t...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da9510cf-2ca9-40d0-bf9b-15d57379dfe6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>processed_summaries</th>\n",
              "      <th>tokenized_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>stereo matching is one of the widely used tech...</td>\n",
              "      <td>[stereo, matching, is, one, of, the, widely, u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the recent advancements in artificial intellig...</td>\n",
              "      <td>[the, recent, advancements, in, artificial, in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in this paper we proposed a novel mutual consi...</td>\n",
              "      <td>[in, this, paper, we, proposed, a, novel, mutu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>consistency training has proven to be an advan...</td>\n",
              "      <td>[consistency, training, has, proven, to, be, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>to ensure safety in automated driving the corr...</td>\n",
              "      <td>[to, ensure, safety, in, automated, driving, t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da9510cf-2ca9-40d0-bf9b-15d57379dfe6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-da9510cf-2ca9-40d0-bf9b-15d57379dfe6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-da9510cf-2ca9-40d0-bf9b-15d57379dfe6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df[['processed_summaries', 'tokenized_summaries']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"processed_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"the recent advancements in artificial intelligence ai combined with the extensive amount of data generated by todays clinical systems has led to the development of imaging ai solutions across the whole value chain of medical imaging including image reconstruction medical image segmentation imagebased diagnosis and treatment planning notwithstanding the successes and future potential of ai in medical imaging many stakeholders are concerned of the potential risks and ethical implications of imaging ai solutions which are perceived as complex opaque and difficult to comprehend utilise and trust in critical clinical applications despite these concerns and risks there are currently no concrete guidelines and best practices for guiding future ai developments in medical imaging towards increased trust safety and adoption to bridge this gap this paper introduces a careful selection of guiding principles drawn from the accumulated experiences consensus and best practices from five large european projects on ai in health imaging these guiding principles are named futureai and its building blocks consist of i fairness ii universality iii traceability iv usability v robustness and vi explainability in a stepbystep approach these guidelines are further translated into a framework of concrete recommendations for specifying developing evaluating and deploying technically clinically and ethically trustworthy ai solutions into clinical practice\",\n          \"to ensure safety in automated driving the correct perception of the situation inside the car is as important as its environment thus seat occupancy detection and classification of detected instances play an important role in interior sensing by the knowledge of the seat occupancy status it is possible to eg automate the airbag deployment control furthermore the presence of a driver which is necessary for partially automated driving cars at the automation levels two to four can be verified in this work we compare different statistical methods from the field of image segmentation to approach the problem of backgroundforeground segmentation in camera based interior sensing in the recent years several methods based on different techniques have been developed and applied to images or videos from different applications the peculiarity of the given scenarios of interior sensing is that the foreground instances and the background both contain static as well as dynamic elements in data considered in this work even the camera position is not completely fixed we review and benchmark three different methods ranging ie gaussian mixture models gmm morphological snakes and a deep neural network namely a mask rcnn in particular the limitations of the classical methods gmm and morphological snakes for interior sensing are shown furthermore it turns that it is possible to overcome these limitations by deep learning eg using a mask rcnn although only a small amount of ground truth data was available for training we enabled the mask rcnn to produce high quality backgroundforeground masks via transfer learning moreover we demonstrate that certain augmentation as well as pre and postprocessing methods further enhance the performance of the investigated methods\",\n          \"in this paper we proposed a novel mutual consistency network mcnet to effectively exploit the unlabeled hard regions for semisupervised medical image segmentation the mcnet model is motivated by the observation that deep models trained with limited annotations are prone to output highly uncertain and easily misclassified predictions in the ambiguous regions eg adhesive edges or thin branches for the image segmentation task leveraging these regionlevel challenging samples can make the semisupervised segmentation model training more effective therefore our proposed mcnet model consists of two new designs first the model contains one shared encoder and multiple sightly different decoders ie using different upsampling strategies the statistical discrepancy of multiple decoders outputs is computed to denote the models uncertainty which indicates the unlabeled hard regions second a new mutual consistency constraint is enforced between one decoders probability output and other decoders soft pseudo labels in this way we minimize the models uncertainty during training and force the model to generate invariant and lowentropy results in such challenging areas of unlabeled data in order to learn a generalized feature representation we compared the segmentation results of the mcnet with five stateoftheart semisupervised approaches on three public medical datasets extension experiments with two common semisupervised settings demonstrate the superior performance of our model over other existing methods which sets a new state of the art for semisupervised medical image segmentation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2f85bd46",
        "outputId": "7a7b9dd3-bc31-4a17-82f6-9f017d340164"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "df['filtered_summaries'] = df['tokenized_summaries'].apply(remove_stopwords)\n",
        "display(df[['tokenized_summaries', 'filtered_summaries']].head())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                 tokenized_summaries  \\\n",
              "0  [stereo, matching, is, one, of, the, widely, u...   \n",
              "1  [the, recent, advancements, in, artificial, in...   \n",
              "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
              "3  [consistency, training, has, proven, to, be, a...   \n",
              "4  [to, ensure, safety, in, automated, driving, t...   \n",
              "\n",
              "                                  filtered_summaries  \n",
              "0  [stereo, matching, one, widely, used, techniqu...  \n",
              "1  [recent, advancements, artificial, intelligenc...  \n",
              "2  [paper, proposed, novel, mutual, consistency, ...  \n",
              "3  [consistency, training, proven, advanced, semi...  \n",
              "4  [ensure, safety, automated, driving, correct, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1547df39-7386-44a5-9f29-586a2bc7930d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokenized_summaries</th>\n",
              "      <th>filtered_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[stereo, matching, is, one, of, the, widely, u...</td>\n",
              "      <td>[stereo, matching, one, widely, used, techniqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[the, recent, advancements, in, artificial, in...</td>\n",
              "      <td>[recent, advancements, artificial, intelligenc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[in, this, paper, we, proposed, a, novel, mutu...</td>\n",
              "      <td>[paper, proposed, novel, mutual, consistency, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[consistency, training, has, proven, to, be, a...</td>\n",
              "      <td>[consistency, training, proven, advanced, semi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[to, ensure, safety, in, automated, driving, t...</td>\n",
              "      <td>[ensure, safety, automated, driving, correct, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1547df39-7386-44a5-9f29-586a2bc7930d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1547df39-7386-44a5-9f29-586a2bc7930d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1547df39-7386-44a5-9f29-586a2bc7930d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df[['tokenized_summaries', 'filtered_summaries']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"tokenized_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filtered_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aa90a16f",
        "outputId": "daaf4fdf-cba4-44f3-8d03-64468a5f3f39"
      },
      "source": [
        "df['clean_summaries'] = df['lemmatized_summaries'].apply(lambda tokens: ' '.join(tokens))\n",
        "display(df[['lemmatized_summaries', 'clean_summaries']].head())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                lemmatized_summaries  \\\n",
              "0  [stereo, matching, one, widely, used, techniqu...   \n",
              "1  [recent, advancement, artificial, intelligence...   \n",
              "2  [paper, proposed, novel, mutual, consistency, ...   \n",
              "3  [consistency, training, proven, advanced, semi...   \n",
              "4  [ensure, safety, automated, driving, correct, ...   \n",
              "\n",
              "                                     clean_summaries  \n",
              "0  stereo matching one widely used technique infe...  \n",
              "1  recent advancement artificial intelligence ai ...  \n",
              "2  paper proposed novel mutual consistency networ...  \n",
              "3  consistency training proven advanced semisuper...  \n",
              "4  ensure safety automated driving correct percep...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4873532d-fc4c-458c-8bcc-c6c09df14dd1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemmatized_summaries</th>\n",
              "      <th>clean_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[stereo, matching, one, widely, used, techniqu...</td>\n",
              "      <td>stereo matching one widely used technique infe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[recent, advancement, artificial, intelligence...</td>\n",
              "      <td>recent advancement artificial intelligence ai ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[paper, proposed, novel, mutual, consistency, ...</td>\n",
              "      <td>paper proposed novel mutual consistency networ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[consistency, training, proven, advanced, semi...</td>\n",
              "      <td>consistency training proven advanced semisuper...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ensure, safety, automated, driving, correct, ...</td>\n",
              "      <td>ensure safety automated driving correct percep...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4873532d-fc4c-458c-8bcc-c6c09df14dd1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4873532d-fc4c-458c-8bcc-c6c09df14dd1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4873532d-fc4c-458c-8bcc-c6c09df14dd1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df[['lemmatized_summaries', 'clean_summaries']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"lemmatized_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"recent advancement artificial intelligence ai combined extensive amount data generated today clinical system led development imaging ai solution across whole value chain medical imaging including image reconstruction medical image segmentation imagebased diagnosis treatment planning notwithstanding success future potential ai medical imaging many stakeholder concerned potential risk ethical implication imaging ai solution perceived complex opaque difficult comprehend utilise trust critical clinical application despite concern risk currently concrete guideline best practice guiding future ai development medical imaging towards increased trust safety adoption bridge gap paper introduces careful selection guiding principle drawn accumulated experience consensus best practice five large european project ai health imaging guiding principle named futureai building block consist fairness ii universality iii traceability iv usability v robustness vi explainability stepbystep approach guideline translated framework concrete recommendation specifying developing evaluating deploying technically clinically ethically trustworthy ai solution clinical practice\",\n          \"ensure safety automated driving correct perception situation inside car important environment thus seat occupancy detection classification detected instance play important role interior sensing knowledge seat occupancy status possible eg automate airbag deployment control furthermore presence driver necessary partially automated driving car automation level two four verified work compare different statistical method field image segmentation approach problem backgroundforeground segmentation camera based interior sensing recent year several method based different technique developed applied image video different application peculiarity given scenario interior sensing foreground instance background contain static well dynamic element data considered work even camera position completely fixed review benchmark three different method ranging ie gaussian mixture model gmm morphological snake deep neural network namely mask rcnn particular limitation classical method gmm morphological snake interior sensing shown furthermore turn possible overcome limitation deep learning eg using mask rcnn although small amount ground truth data available training enabled mask rcnn produce high quality backgroundforeground mask via transfer learning moreover demonstrate certain augmentation well pre postprocessing method enhance performance investigated method\",\n          \"paper proposed novel mutual consistency network mcnet effectively exploit unlabeled hard region semisupervised medical image segmentation mcnet model motivated observation deep model trained limited annotation prone output highly uncertain easily misclassified prediction ambiguous region eg adhesive edge thin branch image segmentation task leveraging regionlevel challenging sample make semisupervised segmentation model training effective therefore proposed mcnet model consists two new design first model contains one shared encoder multiple sightly different decoder ie using different upsampling strategy statistical discrepancy multiple decoder output computed denote model uncertainty indicates unlabeled hard region second new mutual consistency constraint enforced one decoder probability output decoder soft pseudo label way minimize model uncertainty training force model generate invariant lowentropy result challenging area unlabeled data order learn generalized feature representation compared segmentation result mcnet five stateoftheart semisupervised approach three public medical datasets extension experiment two common semisupervised setting demonstrate superior performance model existing method set new state art semisupervised medical image segmentation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "3bdf1048",
        "outputId": "73fd6e39-059b-452b-a92a-b78e722c789b"
      },
      "source": [
        "# Ensure all necessary NLTK data are downloaded\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True) # Open Multilingual Wordnet for better lemmatization support\n",
        "\n",
        "def nltk_preprocessing_pipeline(text):\n",
        "    # Initial Text Preprocessing\n",
        "    def preprocess_text(text):\n",
        "        text = re.sub(r'http\\S+|https\\S+|www\\S+', '', text) # Remove URLs\n",
        "        text = re.sub(r'<.*?>', '', text) # Remove HTML tags\n",
        "        text = re.sub(r'@\\w+', '', text) # Remove social media mentions\n",
        "        text = re.sub(r'#\\w+', '', text) # Remove hashtags\n",
        "        text = text.lower() # Convert to lowercase\n",
        "        emoji_pattern = re.compile(\n",
        "            \"[\" # Start of character group\n",
        "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            \"\\U00002702-\\U000027B0\"\n",
        "            \"\\U000024C2-\\U0001F251\"\n",
        "            \"]+\", flags=re.UNICODE\n",
        "        )\n",
        "        text = emoji_pattern.sub(r'', text)\n",
        "        text = re.sub(r'[^a-z0-9\\s]', '', text) # Remove special characters\n",
        "        text = re.sub(r'\\s+', ' ', text).strip() # Normalize whitespace\n",
        "        return text\n",
        "\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    # Word Tokenization\n",
        "    tokenized_words = nltk.word_tokenize(processed_text)\n",
        "\n",
        "    # Stopword Removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in tokenized_words if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "    # Rejoin words\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Apply the unified pipeline to the original 'summaries' column\n",
        "df['clean_summaries_pipeline'] = df['summaries'].apply(nltk_preprocessing_pipeline)\n",
        "\n",
        "# Display to compare with the step-by-step result\n",
        "display(df[['summaries', 'clean_summaries', 'clean_summaries_pipeline']].head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                                     clean_summaries  \\\n",
              "0  stereo matching one widely used technique infe...   \n",
              "1  recent advancement artificial intelligence ai ...   \n",
              "2  paper proposed novel mutual consistency networ...   \n",
              "3  consistency training proven advanced semisuper...   \n",
              "4  ensure safety automated driving correct percep...   \n",
              "\n",
              "                            clean_summaries_pipeline  \n",
              "0  stereo matching one widely used technique infe...  \n",
              "1  recent advancement artificial intelligence ai ...  \n",
              "2  paper proposed novel mutual consistency networ...  \n",
              "3  consistency training proven advanced semisuper...  \n",
              "4  ensure safety automated driving correct percep...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5083ad5f-9f1f-432b-93d1-b327983182ed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summaries</th>\n",
              "      <th>clean_summaries</th>\n",
              "      <th>clean_summaries_pipeline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>stereo matching one widely used technique infe...</td>\n",
              "      <td>stereo matching one widely used technique infe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>recent advancement artificial intelligence ai ...</td>\n",
              "      <td>recent advancement artificial intelligence ai ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>paper proposed novel mutual consistency networ...</td>\n",
              "      <td>paper proposed novel mutual consistency networ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>consistency training proven advanced semisuper...</td>\n",
              "      <td>consistency training proven advanced semisuper...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>ensure safety automated driving correct percep...</td>\n",
              "      <td>ensure safety automated driving correct percep...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5083ad5f-9f1f-432b-93d1-b327983182ed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5083ad5f-9f1f-432b-93d1-b327983182ed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5083ad5f-9f1f-432b-93d1-b327983182ed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df[['summaries', 'clean_summaries', 'clean_summaries_pipeline']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.\",\n          \"To ensure safety in automated driving, the correct perception of the\\nsituation inside the car is as important as its environment. Thus, seat\\noccupancy detection and classification of detected instances play an important\\nrole in interior sensing. By the knowledge of the seat occupancy status, it is\\npossible to, e.g., automate the airbag deployment control. Furthermore, the\\npresence of a driver, which is necessary for partially automated driving cars\\nat the automation levels two to four can be verified. In this work, we compare\\ndifferent statistical methods from the field of image segmentation to approach\\nthe problem of background-foreground segmentation in camera based interior\\nsensing. In the recent years, several methods based on different techniques\\nhave been developed and applied to images or videos from different\\napplications. The peculiarity of the given scenarios of interior sensing is,\\nthat the foreground instances and the background both contain static as well as\\ndynamic elements. In data considered in this work, even the camera position is\\nnot completely fixed. We review and benchmark three different methods ranging,\\ni.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural\\nnetwork, namely a Mask R-CNN. In particular, the limitations of the classical\\nmethods, GMM and Morphological Snakes, for interior sensing are shown.\\nFurthermore, it turns, that it is possible to overcome these limitations by\\ndeep learning, e.g.\\\\ using a Mask R-CNN. Although only a small amount of ground\\ntruth data was available for training, we enabled the Mask R-CNN to produce\\nhigh quality background-foreground masks via transfer learning. Moreover, we\\ndemonstrate that certain augmentation as well as pre- and post-processing\\nmethods further enhance the performance of the investigated methods.\",\n          \"In this paper, we proposed a novel mutual consistency network (MC-Net+) to\\neffectively exploit the unlabeled hard regions for semi-supervised medical\\nimage segmentation. The MC-Net+ model is motivated by the observation that deep\\nmodels trained with limited annotations are prone to output highly uncertain\\nand easily mis-classified predictions in the ambiguous regions (e.g. adhesive\\nedges or thin branches) for the image segmentation task. Leveraging these\\nregion-level challenging samples can make the semi-supervised segmentation\\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\\nof two new designs. First, the model contains one shared encoder and multiple\\nsightly different decoders (i.e. using different up-sampling strategies). The\\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, a new\\nmutual consistency constraint is enforced between one decoder's probability\\noutput and other decoders' soft pseudo labels. In this way, we minimize the\\nmodel's uncertainty during training and force the model to generate invariant\\nand low-entropy results in such challenging areas of unlabeled data, in order\\nto learn a generalized feature representation. We compared the segmentation\\nresults of the MC-Net+ with five state-of-the-art semi-supervised approaches on\\nthree public medical datasets. Extension experiments with two common\\nsemi-supervised settings demonstrate the superior performance of our model over\\nother existing methods, which sets a new state of the art for semi-supervised\\nmedical image segmentation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"recent advancement artificial intelligence ai combined extensive amount data generated today clinical system led development imaging ai solution across whole value chain medical imaging including image reconstruction medical image segmentation imagebased diagnosis treatment planning notwithstanding success future potential ai medical imaging many stakeholder concerned potential risk ethical implication imaging ai solution perceived complex opaque difficult comprehend utilise trust critical clinical application despite concern risk currently concrete guideline best practice guiding future ai development medical imaging towards increased trust safety adoption bridge gap paper introduces careful selection guiding principle drawn accumulated experience consensus best practice five large european project ai health imaging guiding principle named futureai building block consist fairness ii universality iii traceability iv usability v robustness vi explainability stepbystep approach guideline translated framework concrete recommendation specifying developing evaluating deploying technically clinically ethically trustworthy ai solution clinical practice\",\n          \"ensure safety automated driving correct perception situation inside car important environment thus seat occupancy detection classification detected instance play important role interior sensing knowledge seat occupancy status possible eg automate airbag deployment control furthermore presence driver necessary partially automated driving car automation level two four verified work compare different statistical method field image segmentation approach problem backgroundforeground segmentation camera based interior sensing recent year several method based different technique developed applied image video different application peculiarity given scenario interior sensing foreground instance background contain static well dynamic element data considered work even camera position completely fixed review benchmark three different method ranging ie gaussian mixture model gmm morphological snake deep neural network namely mask rcnn particular limitation classical method gmm morphological snake interior sensing shown furthermore turn possible overcome limitation deep learning eg using mask rcnn although small amount ground truth data available training enabled mask rcnn produce high quality backgroundforeground mask via transfer learning moreover demonstrate certain augmentation well pre postprocessing method enhance performance investigated method\",\n          \"paper proposed novel mutual consistency network mcnet effectively exploit unlabeled hard region semisupervised medical image segmentation mcnet model motivated observation deep model trained limited annotation prone output highly uncertain easily misclassified prediction ambiguous region eg adhesive edge thin branch image segmentation task leveraging regionlevel challenging sample make semisupervised segmentation model training effective therefore proposed mcnet model consists two new design first model contains one shared encoder multiple sightly different decoder ie using different upsampling strategy statistical discrepancy multiple decoder output computed denote model uncertainty indicates unlabeled hard region second new mutual consistency constraint enforced one decoder probability output decoder soft pseudo label way minimize model uncertainty training force model generate invariant lowentropy result challenging area unlabeled data order learn generalized feature representation compared segmentation result mcnet five stateoftheart semisupervised approach three public medical datasets extension experiment two common semisupervised setting demonstrate superior performance model existing method set new state art semisupervised medical image segmentation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_summaries_pipeline\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"recent advancement artificial intelligence ai combined extensive amount data generated today clinical system led development imaging ai solution across whole value chain medical imaging including image reconstruction medical image segmentation imagebased diagnosis treatment planning notwithstanding success future potential ai medical imaging many stakeholder concerned potential risk ethical implication imaging ai solution perceived complex opaque difficult comprehend utilise trust critical clinical application despite concern risk currently concrete guideline best practice guiding future ai development medical imaging towards increased trust safety adoption bridge gap paper introduces careful selection guiding principle drawn accumulated experience consensus best practice five large european project ai health imaging guiding principle named futureai building block consist fairness ii universality iii traceability iv usability v robustness vi explainability stepbystep approach guideline translated framework concrete recommendation specifying developing evaluating deploying technically clinically ethically trustworthy ai solution clinical practice\",\n          \"ensure safety automated driving correct perception situation inside car important environment thus seat occupancy detection classification detected instance play important role interior sensing knowledge seat occupancy status possible eg automate airbag deployment control furthermore presence driver necessary partially automated driving car automation level two four verified work compare different statistical method field image segmentation approach problem backgroundforeground segmentation camera based interior sensing recent year several method based different technique developed applied image video different application peculiarity given scenario interior sensing foreground instance background contain static well dynamic element data considered work even camera position completely fixed review benchmark three different method ranging ie gaussian mixture model gmm morphological snake deep neural network namely mask rcnn particular limitation classical method gmm morphological snake interior sensing shown furthermore turn possible overcome limitation deep learning eg using mask rcnn although small amount ground truth data available training enabled mask rcnn produce high quality backgroundforeground mask via transfer learning moreover demonstrate certain augmentation well pre postprocessing method enhance performance investigated method\",\n          \"paper proposed novel mutual consistency network mcnet effectively exploit unlabeled hard region semisupervised medical image segmentation mcnet model motivated observation deep model trained limited annotation prone output highly uncertain easily misclassified prediction ambiguous region eg adhesive edge thin branch image segmentation task leveraging regionlevel challenging sample make semisupervised segmentation model training effective therefore proposed mcnet model consists two new design first model contains one shared encoder multiple sightly different decoder ie using different upsampling strategy statistical discrepancy multiple decoder output computed denote model uncertainty indicates unlabeled hard region second new mutual consistency constraint enforced one decoder probability output decoder soft pseudo label way minimize model uncertainty training force model generate invariant lowentropy result challenging area unlabeled data order learn generalized feature representation compared segmentation result mcnet five stateoftheart semisupervised approach three public medical datasets extension experiment two common semisupervised setting demonstrate superior performance model existing method set new state art semisupervised medical image segmentation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "72660c3f",
        "outputId": "9fb6f6b9-5b32-4024-c1e6-1f0c3a65fff8"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_words(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "df['lemmatized_summaries'] = df['filtered_summaries'].apply(lemmatize_words)\n",
        "display(df[['filtered_summaries', 'lemmatized_summaries']].head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                  filtered_summaries  \\\n",
              "0  [stereo, matching, one, widely, used, techniqu...   \n",
              "1  [recent, advancements, artificial, intelligenc...   \n",
              "2  [paper, proposed, novel, mutual, consistency, ...   \n",
              "3  [consistency, training, proven, advanced, semi...   \n",
              "4  [ensure, safety, automated, driving, correct, ...   \n",
              "\n",
              "                                lemmatized_summaries  \n",
              "0  [stereo, matching, one, widely, used, techniqu...  \n",
              "1  [recent, advancement, artificial, intelligence...  \n",
              "2  [paper, proposed, novel, mutual, consistency, ...  \n",
              "3  [consistency, training, proven, advanced, semi...  \n",
              "4  [ensure, safety, automated, driving, correct, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0b283e3-1f70-4664-b281-27c5f3a6d1c7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filtered_summaries</th>\n",
              "      <th>lemmatized_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[stereo, matching, one, widely, used, techniqu...</td>\n",
              "      <td>[stereo, matching, one, widely, used, techniqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[recent, advancements, artificial, intelligenc...</td>\n",
              "      <td>[recent, advancement, artificial, intelligence...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[paper, proposed, novel, mutual, consistency, ...</td>\n",
              "      <td>[paper, proposed, novel, mutual, consistency, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[consistency, training, proven, advanced, semi...</td>\n",
              "      <td>[consistency, training, proven, advanced, semi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ensure, safety, automated, driving, correct, ...</td>\n",
              "      <td>[ensure, safety, automated, driving, correct, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0b283e3-1f70-4664-b281-27c5f3a6d1c7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b0b283e3-1f70-4664-b281-27c5f3a6d1c7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b0b283e3-1f70-4664-b281-27c5f3a6d1c7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df[['filtered_summaries', 'lemmatized_summaries']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"filtered_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Using spacy"
      ],
      "metadata": {
        "id": "PH5Y5spfrsSp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e41be5c",
        "outputId": "3db09715-1284-4b70-981e-f51c4c190892"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "# If 'en_core_web_sm' is not downloaded, uncomment and run: !python -m spacy download en_core_web_sm\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"spaCy model loaded successfully!\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "d334e517",
        "outputId": "646e9bd2-0379-41c4-ffa9-8703170189b3"
      },
      "source": [
        "# Load the dataset\n",
        "df_spacy = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "display(df_spacy.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \n",
              "0           ['cs.CV', 'cs.LG']  \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
              "2           ['cs.CV', 'cs.AI']  \n",
              "3                    ['cs.CV']  \n",
              "4           ['cs.CV', 'cs.LG']  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3727f0f-7529-43e9-bf16-569591e8004f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3727f0f-7529-43e9-bf16-569591e8004f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a3727f0f-7529-43e9-bf16-569591e8004f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a3727f0f-7529-43e9-bf16-569591e8004f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_spacy\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging\",\n          \"Background-Foreground Segmentation for Interior Sensing in Automotive Industry\",\n          \"Enforcing Mutual Consistency of Hard Regions for Semi-supervised Medical Image Segmentation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.\",\n          \"To ensure safety in automated driving, the correct perception of the\\nsituation inside the car is as important as its environment. Thus, seat\\noccupancy detection and classification of detected instances play an important\\nrole in interior sensing. By the knowledge of the seat occupancy status, it is\\npossible to, e.g., automate the airbag deployment control. Furthermore, the\\npresence of a driver, which is necessary for partially automated driving cars\\nat the automation levels two to four can be verified. In this work, we compare\\ndifferent statistical methods from the field of image segmentation to approach\\nthe problem of background-foreground segmentation in camera based interior\\nsensing. In the recent years, several methods based on different techniques\\nhave been developed and applied to images or videos from different\\napplications. The peculiarity of the given scenarios of interior sensing is,\\nthat the foreground instances and the background both contain static as well as\\ndynamic elements. In data considered in this work, even the camera position is\\nnot completely fixed. We review and benchmark three different methods ranging,\\ni.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural\\nnetwork, namely a Mask R-CNN. In particular, the limitations of the classical\\nmethods, GMM and Morphological Snakes, for interior sensing are shown.\\nFurthermore, it turns, that it is possible to overcome these limitations by\\ndeep learning, e.g.\\\\ using a Mask R-CNN. Although only a small amount of ground\\ntruth data was available for training, we enabled the Mask R-CNN to produce\\nhigh quality background-foreground masks via transfer learning. Moreover, we\\ndemonstrate that certain augmentation as well as pre- and post-processing\\nmethods further enhance the performance of the investigated methods.\",\n          \"In this paper, we proposed a novel mutual consistency network (MC-Net+) to\\neffectively exploit the unlabeled hard regions for semi-supervised medical\\nimage segmentation. The MC-Net+ model is motivated by the observation that deep\\nmodels trained with limited annotations are prone to output highly uncertain\\nand easily mis-classified predictions in the ambiguous regions (e.g. adhesive\\nedges or thin branches) for the image segmentation task. Leveraging these\\nregion-level challenging samples can make the semi-supervised segmentation\\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\\nof two new designs. First, the model contains one shared encoder and multiple\\nsightly different decoders (i.e. using different up-sampling strategies). The\\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, a new\\nmutual consistency constraint is enforced between one decoder's probability\\noutput and other decoders' soft pseudo labels. In this way, we minimize the\\nmodel's uncertainty during training and force the model to generate invariant\\nand low-entropy results in such challenging areas of unlabeled data, in order\\nto learn a generalized feature representation. We compared the segmentation\\nresults of the MC-Net+ with five state-of-the-art semi-supervised approaches on\\nthree public medical datasets. Extension experiments with two common\\nsemi-supervised settings demonstrate the superior performance of our model over\\nother existing methods, which sets a new state of the art for semi-supervised\\nmedical image segmentation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"['cs.CV', 'cs.AI', 'cs.LG']\",\n          \"['cs.CV']\",\n          \"['cs.CV', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f75036f8",
        "outputId": "4bd18c5e-bc10-4725-b3cb-9ca6ae316833"
      },
      "source": [
        "def preprocess_text_spacy(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|https\\S+|www\\S+', '', text)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove social media mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove hashtags\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove emojis (comprehensive pattern)\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\" # Start of character group\n",
        "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE\n",
        "    )\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    # Remove special characters (keep alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df_spacy['processed_summaries_spacy'] = df_spacy['summaries'].apply(preprocess_text_spacy)\n",
        "display(df_spacy[['summaries', 'processed_summaries_spacy']].head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                           processed_summaries_spacy  \n",
              "0  stereo matching is one of the widely used tech...  \n",
              "1  the recent advancements in artificial intellig...  \n",
              "2  in this paper we proposed a novel mutual consi...  \n",
              "3  consistency training has proven to be an advan...  \n",
              "4  to ensure safety in automated driving the corr...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-19980c2e-5f84-49f9-b398-172827aab616\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summaries</th>\n",
              "      <th>processed_summaries_spacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>stereo matching is one of the widely used tech...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>the recent advancements in artificial intellig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>in this paper we proposed a novel mutual consi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>consistency training has proven to be an advan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>to ensure safety in automated driving the corr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19980c2e-5f84-49f9-b398-172827aab616')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-19980c2e-5f84-49f9-b398-172827aab616 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-19980c2e-5f84-49f9-b398-172827aab616');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_spacy[['summaries', 'processed_summaries_spacy']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.\",\n          \"To ensure safety in automated driving, the correct perception of the\\nsituation inside the car is as important as its environment. Thus, seat\\noccupancy detection and classification of detected instances play an important\\nrole in interior sensing. By the knowledge of the seat occupancy status, it is\\npossible to, e.g., automate the airbag deployment control. Furthermore, the\\npresence of a driver, which is necessary for partially automated driving cars\\nat the automation levels two to four can be verified. In this work, we compare\\ndifferent statistical methods from the field of image segmentation to approach\\nthe problem of background-foreground segmentation in camera based interior\\nsensing. In the recent years, several methods based on different techniques\\nhave been developed and applied to images or videos from different\\napplications. The peculiarity of the given scenarios of interior sensing is,\\nthat the foreground instances and the background both contain static as well as\\ndynamic elements. In data considered in this work, even the camera position is\\nnot completely fixed. We review and benchmark three different methods ranging,\\ni.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural\\nnetwork, namely a Mask R-CNN. In particular, the limitations of the classical\\nmethods, GMM and Morphological Snakes, for interior sensing are shown.\\nFurthermore, it turns, that it is possible to overcome these limitations by\\ndeep learning, e.g.\\\\ using a Mask R-CNN. Although only a small amount of ground\\ntruth data was available for training, we enabled the Mask R-CNN to produce\\nhigh quality background-foreground masks via transfer learning. Moreover, we\\ndemonstrate that certain augmentation as well as pre- and post-processing\\nmethods further enhance the performance of the investigated methods.\",\n          \"In this paper, we proposed a novel mutual consistency network (MC-Net+) to\\neffectively exploit the unlabeled hard regions for semi-supervised medical\\nimage segmentation. The MC-Net+ model is motivated by the observation that deep\\nmodels trained with limited annotations are prone to output highly uncertain\\nand easily mis-classified predictions in the ambiguous regions (e.g. adhesive\\nedges or thin branches) for the image segmentation task. Leveraging these\\nregion-level challenging samples can make the semi-supervised segmentation\\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\\nof two new designs. First, the model contains one shared encoder and multiple\\nsightly different decoders (i.e. using different up-sampling strategies). The\\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, a new\\nmutual consistency constraint is enforced between one decoder's probability\\noutput and other decoders' soft pseudo labels. In this way, we minimize the\\nmodel's uncertainty during training and force the model to generate invariant\\nand low-entropy results in such challenging areas of unlabeled data, in order\\nto learn a generalized feature representation. We compared the segmentation\\nresults of the MC-Net+ with five state-of-the-art semi-supervised approaches on\\nthree public medical datasets. Extension experiments with two common\\nsemi-supervised settings demonstrate the superior performance of our model over\\nother existing methods, which sets a new state of the art for semi-supervised\\nmedical image segmentation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_summaries_spacy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"the recent advancements in artificial intelligence ai combined with the extensive amount of data generated by todays clinical systems has led to the development of imaging ai solutions across the whole value chain of medical imaging including image reconstruction medical image segmentation imagebased diagnosis and treatment planning notwithstanding the successes and future potential of ai in medical imaging many stakeholders are concerned of the potential risks and ethical implications of imaging ai solutions which are perceived as complex opaque and difficult to comprehend utilise and trust in critical clinical applications despite these concerns and risks there are currently no concrete guidelines and best practices for guiding future ai developments in medical imaging towards increased trust safety and adoption to bridge this gap this paper introduces a careful selection of guiding principles drawn from the accumulated experiences consensus and best practices from five large european projects on ai in health imaging these guiding principles are named futureai and its building blocks consist of i fairness ii universality iii traceability iv usability v robustness and vi explainability in a stepbystep approach these guidelines are further translated into a framework of concrete recommendations for specifying developing evaluating and deploying technically clinically and ethically trustworthy ai solutions into clinical practice\",\n          \"to ensure safety in automated driving the correct perception of the situation inside the car is as important as its environment thus seat occupancy detection and classification of detected instances play an important role in interior sensing by the knowledge of the seat occupancy status it is possible to eg automate the airbag deployment control furthermore the presence of a driver which is necessary for partially automated driving cars at the automation levels two to four can be verified in this work we compare different statistical methods from the field of image segmentation to approach the problem of backgroundforeground segmentation in camera based interior sensing in the recent years several methods based on different techniques have been developed and applied to images or videos from different applications the peculiarity of the given scenarios of interior sensing is that the foreground instances and the background both contain static as well as dynamic elements in data considered in this work even the camera position is not completely fixed we review and benchmark three different methods ranging ie gaussian mixture models gmm morphological snakes and a deep neural network namely a mask rcnn in particular the limitations of the classical methods gmm and morphological snakes for interior sensing are shown furthermore it turns that it is possible to overcome these limitations by deep learning eg using a mask rcnn although only a small amount of ground truth data was available for training we enabled the mask rcnn to produce high quality backgroundforeground masks via transfer learning moreover we demonstrate that certain augmentation as well as pre and postprocessing methods further enhance the performance of the investigated methods\",\n          \"in this paper we proposed a novel mutual consistency network mcnet to effectively exploit the unlabeled hard regions for semisupervised medical image segmentation the mcnet model is motivated by the observation that deep models trained with limited annotations are prone to output highly uncertain and easily misclassified predictions in the ambiguous regions eg adhesive edges or thin branches for the image segmentation task leveraging these regionlevel challenging samples can make the semisupervised segmentation model training more effective therefore our proposed mcnet model consists of two new designs first the model contains one shared encoder and multiple sightly different decoders ie using different upsampling strategies the statistical discrepancy of multiple decoders outputs is computed to denote the models uncertainty which indicates the unlabeled hard regions second a new mutual consistency constraint is enforced between one decoders probability output and other decoders soft pseudo labels in this way we minimize the models uncertainty during training and force the model to generate invariant and lowentropy results in such challenging areas of unlabeled data in order to learn a generalized feature representation we compared the segmentation results of the mcnet with five stateoftheart semisupervised approaches on three public medical datasets extension experiments with two common semisupervised settings demonstrate the superior performance of our model over other existing methods which sets a new state of the art for semisupervised medical image segmentation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "635d578b"
      },
      "source": [
        "### 3. spaCy Tokenization, Stopword Removal, and Lemmatization\n",
        "Using spaCy's NLP pipeline to perform tokenization, remove stopwords, and lemmatize words in a single pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "170ed6f6",
        "outputId": "49dd89ab-a040-48fb-ce0d-adc51cff43dc"
      },
      "source": [
        "def spacy_pipeline(text):\n",
        "    doc = nlp(text)\n",
        "    # Filter out stopwords, punctuation, and non-alphabetic tokens\n",
        "    lemmas = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "df_spacy['clean_summaries_spacy'] = df_spacy['processed_summaries_spacy'].apply(spacy_pipeline)\n",
        "display(df_spacy[['summaries', 'clean_summaries_spacy']].head())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                               clean_summaries_spacy  \n",
              "0  stereo matching widely technique infer depth s...  \n",
              "1  recent advancement artificial intelligence ai ...  \n",
              "2  paper propose novel mutual consistency network...  \n",
              "3  consistency training prove advanced semisuperv...  \n",
              "4  ensure safety automated drive correct percepti...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f9d2cea-823c-425c-a131-5828515cf128\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summaries</th>\n",
              "      <th>clean_summaries_spacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>stereo matching widely technique infer depth s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>recent advancement artificial intelligence ai ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>paper propose novel mutual consistency network...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>consistency training prove advanced semisuperv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>ensure safety automated drive correct percepti...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f9d2cea-823c-425c-a131-5828515cf128')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1f9d2cea-823c-425c-a131-5828515cf128 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1f9d2cea-823c-425c-a131-5828515cf128');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_spacy[['summaries', 'clean_summaries_spacy']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.\",\n          \"To ensure safety in automated driving, the correct perception of the\\nsituation inside the car is as important as its environment. Thus, seat\\noccupancy detection and classification of detected instances play an important\\nrole in interior sensing. By the knowledge of the seat occupancy status, it is\\npossible to, e.g., automate the airbag deployment control. Furthermore, the\\npresence of a driver, which is necessary for partially automated driving cars\\nat the automation levels two to four can be verified. In this work, we compare\\ndifferent statistical methods from the field of image segmentation to approach\\nthe problem of background-foreground segmentation in camera based interior\\nsensing. In the recent years, several methods based on different techniques\\nhave been developed and applied to images or videos from different\\napplications. The peculiarity of the given scenarios of interior sensing is,\\nthat the foreground instances and the background both contain static as well as\\ndynamic elements. In data considered in this work, even the camera position is\\nnot completely fixed. We review and benchmark three different methods ranging,\\ni.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural\\nnetwork, namely a Mask R-CNN. In particular, the limitations of the classical\\nmethods, GMM and Morphological Snakes, for interior sensing are shown.\\nFurthermore, it turns, that it is possible to overcome these limitations by\\ndeep learning, e.g.\\\\ using a Mask R-CNN. Although only a small amount of ground\\ntruth data was available for training, we enabled the Mask R-CNN to produce\\nhigh quality background-foreground masks via transfer learning. Moreover, we\\ndemonstrate that certain augmentation as well as pre- and post-processing\\nmethods further enhance the performance of the investigated methods.\",\n          \"In this paper, we proposed a novel mutual consistency network (MC-Net+) to\\neffectively exploit the unlabeled hard regions for semi-supervised medical\\nimage segmentation. The MC-Net+ model is motivated by the observation that deep\\nmodels trained with limited annotations are prone to output highly uncertain\\nand easily mis-classified predictions in the ambiguous regions (e.g. adhesive\\nedges or thin branches) for the image segmentation task. Leveraging these\\nregion-level challenging samples can make the semi-supervised segmentation\\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\\nof two new designs. First, the model contains one shared encoder and multiple\\nsightly different decoders (i.e. using different up-sampling strategies). The\\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, a new\\nmutual consistency constraint is enforced between one decoder's probability\\noutput and other decoders' soft pseudo labels. In this way, we minimize the\\nmodel's uncertainty during training and force the model to generate invariant\\nand low-entropy results in such challenging areas of unlabeled data, in order\\nto learn a generalized feature representation. We compared the segmentation\\nresults of the MC-Net+ with five state-of-the-art semi-supervised approaches on\\nthree public medical datasets. Extension experiments with two common\\nsemi-supervised settings demonstrate the superior performance of our model over\\nother existing methods, which sets a new state of the art for semi-supervised\\nmedical image segmentation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_summaries_spacy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"recent advancement artificial intelligence ai combine extensive datum generate today clinical system lead development imaging ai solution value chain medical imaging include image reconstruction medical image segmentation imagebase diagnosis treatment planning notwithstanding success future potential ai medical imaging stakeholder concerned potential risk ethical implication imaging ai solution perceive complex opaque difficult comprehend utilise trust critical clinical application despite concern risk currently concrete guideline good practice guide future ai development medical imaging increase trust safety adoption bridge gap paper introduce careful selection guide principle draw accumulate experience consensus good practice large european project ai health imaging guide principle name futureai building block consist fairness ii universality iii traceability iv usability v robustness vi explainability stepbystep approach guideline translate framework concrete recommendation specify develop evaluating deploy technically clinically ethically trustworthy ai solution clinical practice\",\n          \"ensure safety automated drive correct perception situation inside car important environment seat occupancy detection classification detect instance play important role interior sensing knowledge seat occupancy status possible eg automate airbag deployment control furthermore presence driver necessary partially automate driving car automation level verify work compare different statistical method field image segmentation approach problem backgroundforeground segmentation camera base interior sense recent year method base different technique develop apply image video different application peculiarity give scenario interior sensing foreground instance background contain static dynamic element datum consider work camera position completely fix review benchmark different method range ie gaussian mixture model gmm morphological snake deep neural network mask rcnn particular limitation classical method gmm morphological snake interior sensing show furthermore turn possible overcome limitation deep learning eg mask rcnn small ground truth datum available training enable mask rcnn produce high quality backgroundforeground mask transfer learning demonstrate certain augmentation pre postprocesse method enhance performance investigate method\",\n          \"paper propose novel mutual consistency network mcnet effectively exploit unlabeled hard region semisupervise medical image segmentation mcnet model motivate observation deep model train limited annotation prone output highly uncertain easily misclassifie prediction ambiguous region eg adhesive edge thin branch image segmentation task leverage regionlevel challenging sample semisupervised segmentation model training effective propose mcnet model consist new design model contain shared encoder multiple sightly different decoder ie different upsampling strategy statistical discrepancy multiple decoder output compute denote model uncertainty indicate unlabeled hard region second new mutual consistency constraint enforce decoder probability output decoder soft pseudo label way minimize model uncertainty training force model generate invariant lowentropy result challenging area unlabeled datum order learn generalized feature representation compare segmentation result mcnet stateoftheart semisupervise approach public medical dataset extension experiment common semisupervised setting demonstrate superior performance model exist method set new state art semisupervise medical image segmentation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eda4cf2",
        "outputId": "45e0de8c-430e-4079-a880-908089b2a67b"
      },
      "source": [
        "import nltk\n",
        "import collections\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# Download necessary NLTK data for POS tagging if not already present\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "print(\"NLTK 'averaged_perceptron_tagger' package is ready.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK 'averaged_perceptron_tagger' package is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c91a6987"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary NLTK package is confirmed, I will define a noun phrase grammar, iterate through the lemmatized summaries, perform POS tagging, apply the `RegexpParser` to extract noun phrases, and then count their frequencies to display the top 10.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b3a5fa6",
        "outputId": "c3c77ff9-3a31-4015-b84c-963b93f23c62"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# The noun phrase counts were already calculated and stored in 'np_counts' in the previous cell (0f73cd7b)\n",
        "# We can directly use np_counts here.\n",
        "# If you intend to use df_spacy, you would need to add a 'noun_phrases' column to it first.\n",
        "\n",
        "# Display the 10 most common noun phrases and their counts from the already computed np_counts\n",
        "print(\"Top 10 Most Common Noun Phrases:\")\n",
        "for phrase, count in np_counts.most_common(10):\n",
        "    print(f\"- {phrase}: {count}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most Common Noun Phrases:\n",
            "- method: 272\n",
            "- image: 206\n",
            "- model: 187\n",
            "- approach: 133\n",
            "- data: 131\n",
            "- medical image segmentation: 121\n",
            "- image segmentation: 114\n",
            "- segmentation: 110\n",
            "- semantic segmentation: 103\n",
            "- performance: 91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a249e420",
        "outputId": "0664b244-71c9-4847-e106-51ffc8ae3cae"
      },
      "source": [
        "import collections\n",
        "\n",
        "# Initialize a dictionary to store entities grouped by their type\n",
        "named_entities = collections.defaultdict(list)\n",
        "\n",
        "# Iterate through each processed summary in the df_spacy DataFrame\n",
        "for summary_text in df_spacy['clean_summaries_spacy']:\n",
        "    # Pass the summary through the nlp object to create a doc object\n",
        "    doc = nlp(summary_text)\n",
        "\n",
        "    # Iterate through doc.ents (named entities) for the current document\n",
        "    for ent in doc.ents:\n",
        "        # Append the entity's text to the list corresponding to its type\n",
        "        named_entities[ent.label_].append(ent.text)\n",
        "\n",
        "# After processing all summaries, iterate through the named_entities dictionary\n",
        "print(\"Top 5 Most Frequent Named Entities by Type (spaCy):\")\n",
        "for ent_type, entities in named_entities.items():\n",
        "    # Use collections.Counter to count the frequency of each unique named entity\n",
        "    entity_counts = collections.Counter(entities)\n",
        "\n",
        "    print(f\"\\nEntity Type: {ent_type}\")\n",
        "    # Print the top 5 most frequent named entities for each entity type\n",
        "    for entity, count in entity_counts.most_common(5):\n",
        "        print(f\"- '{entity}': {count}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Frequent Named Entities by Type (spaCy):\n",
            "\n",
            "Entity Type: DATE\n",
            "- 'recent year': 40\n",
            "- 'today': 6\n",
            "- 'past year': 6\n",
            "- 'past decade': 5\n",
            "- 'winter': 4\n",
            "\n",
            "Entity Type: NORP\n",
            "- 'scan': 34\n",
            "- 'cmean': 8\n",
            "- 'kmeans': 3\n",
            "- 'gaussian': 2\n",
            "- 'siamese': 2\n",
            "\n",
            "Entity Type: ORG\n",
            "- 'cnn': 223\n",
            "- 'linear': 12\n",
            "- 'convolutional neural': 11\n",
            "- 'nih': 7\n",
            "- 'node': 5\n",
            "\n",
            "Entity Type: ORDINAL\n",
            "- 'second': 82\n",
            "- 'firstly': 27\n",
            "- 'secondly': 11\n",
            "- 'thirdly': 3\n",
            "- 'fifth': 1\n",
            "\n",
            "Entity Type: PERSON\n",
            "- 'gpu': 23\n",
            "- 'iris': 14\n",
            "- 'iris image': 9\n",
            "- 'multiorgan': 5\n",
            "- 'groundtruth': 5\n",
            "\n",
            "Entity Type: CARDINAL\n",
            "- 'one': 31\n",
            "- 'seven': 8\n",
            "- 'million': 5\n",
            "- 'hundred': 4\n",
            "- 'twofold': 4\n",
            "\n",
            "Entity Type: LANGUAGE\n",
            "- 'scan': 1\n",
            "\n",
            "Entity Type: PRODUCT\n",
            "- 'standard metric uncertaintyaware metric': 2\n",
            "- 'discovery': 1\n",
            "- 'metric meteorological metric': 1\n",
            "- 'simplex': 1\n",
            "- 'minima': 1\n",
            "\n",
            "Entity Type: GPE\n",
            "- 'berkeley': 4\n",
            "- 'node': 4\n",
            "- 'metamoran': 3\n",
            "- 'elastica': 3\n",
            "- 'china': 2\n",
            "\n",
            "Entity Type: TIME\n",
            "- 'daytime': 10\n",
            "- 'hour': 7\n",
            "- 'matter hour minute': 1\n",
            "- 'obtain minute': 1\n",
            "\n",
            "Entity Type: QUANTITY\n",
            "- 'ten meter': 1\n",
            "- 'seven degree freedom': 1\n",
            "\n",
            "Entity Type: PERCENT\n",
            "- 'eighty percent': 1\n",
            "\n",
            "Entity Type: LOC\n",
            "- 'crf base': 1\n",
            "- 'lake ice': 1\n",
            "- 'sse iii': 1\n",
            "- 'north america': 1\n"
          ]
        }
      ]
    }
  ]
}